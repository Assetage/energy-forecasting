Hourly Energy Consumption
==============================
This project based on https://www.kaggle.com/datasets/robikscube/hourly-energy-consumption 
### ðŸ“– Description
### **PJM Hourly Energy Consumption Data**
PJM Interconnection LLC (PJM) is a regional transmission organization (RTO) in the United States. It is part of the Eastern Interconnection grid operating an electric transmission system serving all or parts of Delaware, Illinois, Indiana, Kentucky, Maryland, Michigan, New Jersey, North Carolina, Ohio, Pennsylvania, Tennessee, Virginia, West Virginia, and the District of Columbia.

The hourly power consumption data comes from PJM's website and are in megawatts (MW).

The regions have changed over the years so data may only appear for certain dates per region.
   
A short description of the project.
------------
The primary objective of this project is to establish a well-organized project structure and effectively implement diverse techniques.   
Project structure is based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>.   
For config handling  <a target="_blank" href="https://hydra.cc/docs/intro/">Hydra library</a>  is used .   
Notebooks folder contain notebook done based on the work of <a target="_blank" href="https://www.kaggle.com/code/robikscube/pt2-time-series-forecasting-with-xgboost">PT2: Time Series Forecasting with XGBoost</a> (ToDo: add more features).   

To be done:
1. Add more features
2. Try other models
3. Make better analysis notebook
4. Cover more functions with tests

## **Project Organization**
------------
```plaintext
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ data
â”‚   â””â”€â”€ raw                                     <- The original, immutable data dump.
â”œâ”€â”€ models                                      <- Trained and serialized models, model predictions, or model summaries
â”œâ”€â”€ notebooks                                   <- Jupyter notebooks
â”œâ”€â”€ outputs                                     <- Hydra logs
â”œâ”€â”€ pyproject.toml                              <- .toml file to define a project package and make it installable (pip install -e .)
â”œâ”€â”€ requirements.txt                            <- requirements for correct installation of the project package
â””â”€â”€ src                                         <- Source code for use in this project
     â””â”€â”€ energy_forecasting                      <- Sub-folder of the source code for possible additional applications to the package
         â”œâ”€â”€ __init__.py                         <- Makes src a Python module
         â”œâ”€â”€ __main__.py                         <- Main function which allows to call entire train / optimize / predict pipelines
         â”œâ”€â”€ conf
         â”‚   â”œâ”€â”€ config.yaml
         â”‚   â”œâ”€â”€ feature_params                  <- Configs for features
         â”‚   â”œâ”€â”€ optimizer_params                <- Configs for optimizer
         â”‚   â”œâ”€â”€ path_config                     <- Configs for paths   
         â”‚   â”œâ”€â”€ pipeline                        <- Configs for pipeline
         â”‚   â”œâ”€â”€ splitting_params                <- Configs for splitting params
         â”‚   â””â”€â”€ train_params                    <- Configs for train
         â”œâ”€â”€ data
         â”‚   â”œâ”€â”€ __init__.py
         â”‚   â””â”€â”€ make_dataset.py                 <- Scripts for time series splitting of the dataset
         â”œâ”€â”€ entities                            <- Scripts for creating dataclasses
         â”œâ”€â”€ features                            <- Scripts to turn raw data into features for modeling
         â”œâ”€â”€ model_params_optimizer.py           <- Optimizer pipeline code
         â”œâ”€â”€ models                              <- Scripts to train models and making predictions based on them
         â”‚   â”œâ”€â”€ __init__.py
         â”‚   â”œâ”€â”€ predict_model.py                
         â”‚   â””â”€â”€ train_model.py
         â”œâ”€â”€ predict_pipeline.py                 <- Predict pipeline code
         â”œâ”€â”€ train_pipeline.py                   <- Train pipeline code
         â””â”€â”€ utils
             â”œâ”€â”€ __init__.py
             â””â”€â”€ utils.py                        <- Additional utility scripts
â”œâ”€â”€ tests                                        <- Project tests

```
--------
## â›**Package installation**
The package can be installed by cloning the repo and then by typing: 
```shell
pip install .
```
The package will be available in your pip under `energy-forecasting` name.

--------
## âš¡**Run training**
Once the package is installed, one can initiate a training pipeline. For training the "pipeline" argument value is "train". Take into account that "train_params" argument allows a user to train a specific model. For the current release of the package only "LinearRegression", "RandomForestRegressor" and "MLPRegressor" models are available with "lr", "rf" and "nn" shortcuts, respectively.

Run model train for Random Forest Regressor:
```shell  
energy_forecasting pipeline=train train_params=rf
```

--------
## âš¡**Run prediction**
Once the package is installed, one can initiate a training pipeline. For prediction the "pipeline" argument value is "predict". In order to use a specific model use "predict_params" argument values of "lr", "rf" or "nn".

Run prediction with a Random Forest Regressor:  
```shell
energy_forecasting pipeline=predict predict_params=rf
```

--------
## âš¡**Run optimization**
Once the package is installed, one can initiate a training pipeline. For model optimization the "pipeline" argument value is "optimize". In order to use a specific model to optimize use "optimize_params" argument values of "lr", "rf" or "nn".

Run model optimization for a Random Forest Regressor:  
```shell
energy_forecasting pipeline=predict optimize_params=rf
```
--------
## **Additional information**
To ensure that you follow the development workflow, please setup the pre-commit hooks:

```shell
pre-commit install
```

You can run the tests with the following commands:

```shell
#Â Unit tests
make test
```